## Suggest Gloud to test 

#ss.yml

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-cluster
data:
  update-node.sh: |
    #!/bin/sh
    REDIS_NODES="/data/nodes.conf"
    sed -i -e "/myself/ s/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/${POD_IP}/" ${REDIS_NODES}
    exec "$@"
  redis.conf: |+
    cluster-enabled yes
    cluster-require-full-coverage no
    cluster-node-timeout 15000
    cluster-config-file /data/nodes.conf
    cluster-migration-barrier 1
    appendonly yes
    protected-mode no
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 6
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      containers:
      - name: redis
        image: redis:5.0.1-alpine
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        command: ["/conf/update-node.sh", "redis-server", "/conf/redis.conf"]
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: conf
          mountPath: /conf
          readOnly: false
        - name: data
          mountPath: /data
          readOnly: false
      volumes:
      - name: conf
        configMap:
          name: redis-cluster
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 50Mi
---
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
spec:
  clusterIP: None
  ports:
  - port: 6379
    targetPort: 6379
    name: client
  - port: 16379
    targetPort: 16379
    name: gossip
  selector:
    app: redis-cluster
    
    
    
=====================================================
Get the IPs of pods and save to a variable:
==============

$IPs=$(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379')

echo $IPs to test
=======
10.4.1.8:6379 10.4.0.7:6379 10.4.1.9:6379 10.4.0.8:6379 10.4.1.10:6379 10.4.0.9:6379

Now enter into a first pod for cluster formation
==============================================


kubectl exec -it redis-cluster-0  -- /bin/sh 


Create Cluster:

redis-cli -h 127.0.0.1 -p 6379 --cluster create 10.4.1.8:6379 10.4.0.7:6379 10.4.1.9:6379 10.4.0.8:6379 10.4.1.10:6379 10.4.0.9:6379

Check Cluster Status:

 redis-cli -h 127.0.0.1 -p 6379 cluster info

================================================

NOW Deploy APP with Service :
=============================

apiVersion: v1
kind: Service
metadata:
  name: hit-counter-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
    targetPort: 5000
  selector:
      app: myapp
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hit-counter-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: aimvector/api-redis-ha:1.0
        ports:
        - containerPort: 5000

==================================


Now RUN the app with LoadBalancer Ip:

1. keep on refresh to the see the counter increases the value.
2. Delete couple of Pods and re-test the app  (the counter should continue)
3. Scale down redis statefulset to 4 (the counter should continue)
4. If you scale down to 1 and see cluster info --- it shows failure as kube doesnt take care of redis cluster config.
